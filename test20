# glue_job_parquet_last6months_final_v3.py
import sys
import os
import logging
from datetime import datetime
from dateutil.relativedelta import relativedelta

import boto3
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from pyspark.sql.functions import col, to_date, date_format
from pyspark.sql.types import IntegerType
from awsglue.job import Job
from awsglue.utils import getResolvedOptions

# ============================================================
#  ARGS
# ============================================================
args = getResolvedOptions(
    sys.argv,
    ["JDBC_URL", "DB_USER", "DB_PASSWORD", "S3_OUTPUT"]
)

jdbc_url = args["JDBC_URL"]
db_user = args["DB_USER"]
db_password = args["DB_PASSWORD"]
s3_output = args["S3_OUTPUT"].rstrip("/")
temp_dir = os.environ.get("GLUE_TEMP_DIR", f"{s3_output}/_temp")

# ============================================================
#  LOGGING
# ============================================================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("glue_job")
logger.setLevel(logging.INFO)

# ============================================================
#  INIT
# ============================================================
sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)

logger.info(f"Job iniciado. Output S3: {s3_output}")

# ============================================================
#  CUT DATE (6 MESES)
# ============================================================
six_months_ago = datetime.utcnow() - relativedelta(months=6)
six_months_str = six_months_ago.strftime("%Y-%m-%d %H:%M:%S")

# ============================================================
#  SQL QUERY
# ============================================================
sql = f"""
SELECT
    [num_chamado],
    [equipe],
    [gerente_equipe],
    [dpto_equipe],
    [coord_dpto_equipe],
    [ger_dpto_equipe],
    [dpto_ger_equipe],
    [severidade],
    [dat_abertura],
    [dat_status_concluido],
    [dat_fechamento],
    [data_normalizacao],
    [status],
    [classificacao],
    [dat_estouro_sla],
    [num_duracao],
    [duracao_sla],
    [titulo],
    [sla_violado]
FROM [mdb_rep].[dbo].[sdm_cr_geral_mcs]
WHERE dat_abertura >= '{six_months_str}'
  AND (
        (classificacao = 'Incidente' AND severidade IN ('3 - Média', '4 - Alta', '5 - Crítica'))
     OR (classificacao = 'Solicitação' AND severidade = '4 - Alta')
      )
"""

logger.info(f"Executando SQL (inicio): {sql[:200]}...")

# ============================================================
#  LEITURA DO SQL SERVER
# ============================================================
try:
    raw_df = (
        spark.read.format("jdbc")
        .option("url", jdbc_url)
        .option("user", db_user)
        .option("password", db_password)
        .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
        .option("fetchsize", "10000")
        .option("query", sql)
        .load()
    )

    count_rows = raw_df.count()
    logger.info(f"Linhas retornadas do SQL Server: {count_rows}")

    if count_rows == 0:
        logger.warning("⚠️ Nenhum dado retornado da consulta SQL. Job encerrado sem escrita.")
        job.commit()
        sys.exit(0)

except Exception as e:
    logger.exception("Erro ao ler dados via JDBC:")
    raise e

# ============================================================
#  NORMALIZAÇÃO
# ============================================================
df = raw_df
date_cols = ["dat_abertura", "dat_status_concluido", "dat_fechamento", "data_normalizacao", "dat_estouro_sla"]
for c in date_cols:
    if c in df.columns:
        df = df.withColumn(c, to_date(col(c)))

num_cols = ["num_duracao", "duracao_sla"]
for c in num_cols:
    if c in df.columns:
        df = df.withColumn(c, col(c).cast(IntegerType()))

str_cols = [
    "equipe", "gerente_equipe", "dpto_equipe", "coord_dpto_equipe", "ger_dpto_equipe",
    "dpto_ger_equipe", "severidade", "status", "classificacao", "titulo", "sla_violado"
]
for c in str_cols:
    if c in df.columns:
        df = df.withColumn(c, col(c).cast("string"))

df = df.withColumn("year", date_format(col("dat_abertura"), "yyyy")) \
       .withColumn("month", date_format(col("dat_abertura"), "MM"))

logger.info("Schema final:")
df.printSchema()

# ============================================================
#  ESCRITA PARQUET
# ============================================================
try:
    df = df.coalesce(10)
    logger.info("Gravando dados em Parquet particionado...")

    (
        df.write
        .mode("overwrite")
        .option("compression", "snappy")
        .partitionBy("year", "month")
        .parquet(s3_output)
    )

    logger.info("✅ Escrita concluída com sucesso em: %s", s3_output)

    # Verificação pós-escrita (confirma se há arquivos no bucket)
    s3 = boto3.client("s3")
    bucket_name = s3_output.replace("s3://", "").split("/")[0]
    prefix = "/".join(s3_output.replace("s3://", "").split("/")[1:])
    objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)

    if "Contents" not in objects or len(objects["Contents"]) == 0:
        logger.warning("⚠️ Nenhum arquivo encontrado no S3 após escrita!")
    else:
        logger.info(f"Arquivos gerados: {len(objects['Contents'])}")

except Exception as e:
    logger.exception("Erro ao gravar Parquet no S3:")
    raise e

# ============================================================
#  FIM
# ============================================================
logger.info("Job finalizado com sucesso.")
job.commit()
