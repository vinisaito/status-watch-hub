import boto3
import json
import time
import logging

# --- Config ---
athena = boto3.client("athena", region_name="us-east-1")
s3 = boto3.client("s3")

DATABASE = "painel-monitoracao"
WORKGROUP = "painel_monitoracao"
OUTPUT_BUCKET = "s3://painel-monitoracao-administrativo-dados/athena_lambda/"
CACHE_BUCKET = "painel-monitoracao-administrativo-dados"
CACHE_PREFIX = "api_cache/dados_part_"
INDEX_KEY = "api_cache/index.json"

QUERY = """
SELECT 
    num_chamado,
    equipe,
    gerente_equipe,
    dpto_equipe,
    coord_dpto_equipe,
    ger_dpto_equipe,
    dpto_ger_equipe,
    severidade,
    dat_abertura,
    dat_status_concluido,
    dat_fechamento,
    data_normalizacao,
    status,
    classificacao,
    dat_estouro_sla,
    num_duracao,
    duracao_sla,
    titulo,
    sla_violado
FROM painel_monitoracao_administrativo
WHERE dat_abertura >= date_add('month', -6, current_date)
  AND (
        (classificacao = 'Incidente' AND severidade IN ('3 - Média', '4 - Alta', '5 - Crítica'))
     OR (classificacao = 'Solicitação' AND severidade = '4 - Alta')
      )
ORDER BY num_chamado DESC
"""

logger = logging.getLogger()
logger.setLevel(logging.INFO)

CHUNK_SIZE = 50000  # Número de registros por arquivo

def lambda_handler(event, context):
    logger.info("Iniciando execução Athena...")

    # Executa query
    exec_id = athena.start_query_execution(
        QueryString=QUERY,
        QueryExecutionContext={"Database": DATABASE},
        ResultConfiguration={"OutputLocation": OUTPUT_BUCKET},
        WorkGroup=WORKGROUP
    )["QueryExecutionId"]

    # Aguarda conclusão
    while True:
        status = athena.get_query_execution(QueryExecutionId=exec_id)["QueryExecution"]["Status"]["State"]
        if status in ["SUCCEEDED", "FAILED", "CANCELLED"]:
            break
        time.sleep(3)

    if status != "SUCCEEDED":
        raise Exception(f"Athena query failed: {status}")

    logger.info("Consulta Athena concluída com sucesso.")

    # --- Paginação ---
    headers = []
    results = []
    total_count = 0
    part_number = 1
    s3_files = []
    next_token = None

    while True:
        response = athena.get_query_results(QueryExecutionId=exec_id, NextToken=next_token) if next_token else athena.get_query_results(QueryExecutionId=exec_id)

        if not headers:
            headers = [c["Label"] for c in response["ResultSet"]["ResultSetMetadata"]["ColumnInfo"]]

        for row in response["ResultSet"]["Rows"][1:]:
            values = [v.get("VarCharValue") for v in row["Data"]]
            results.append(dict(zip(headers, values)))
            total_count += 1

            if len(results) >= CHUNK_SIZE:
                key = f"{CACHE_PREFIX}{part_number:03d}.json"
                s3.put_object(
                    Bucket=CACHE_BUCKET,
                    Key=key,
                    Body=json.dumps(results, ensure_ascii=False),
                    ContentType="application/json"
                )
                logger.info(f"Salvou {len(results)} registros em {key}")
                s3_files.append(key)
                results.clear()
                part_number += 1

        next_token = response.get("NextToken")
        if not next_token:
            break

    # Salva últimos registros restantes
    if results:
        key = f"{CACHE_PREFIX}{part_number:03d}.json"
        s3.put_object(
            Bucket=CACHE_BUCKET,
            Key=key,
            Body=json.dumps(results, ensure_ascii=False),
            ContentType="application/json"
        )
        logger.info(f"Salvou {len(results)} registros finais em {key}")
        s3_files.append(key)

    # Salva índice com lista de partes
    index_data = {"parts": len(s3_files), "files": s3_files}
    s3.put_object(
        Bucket=CACHE_BUCKET,
        Key=INDEX_KEY,
        Body=json.dumps(index_data, ensure_ascii=False),
        ContentType="application/json"
    )

    logger.info(f"Processo concluído. Total de {total_count} registros em {len(s3_files)} arquivos.")

    return {
        "status": "ok",
        "total_records": total_count,
        "parts": len(s3_files),
        "s3_files": [f"s3://{CACHE_BUCKET}/{k}" for k in s3_files]
    }
