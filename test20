import boto3
import json
import time
import csv
import io
import logging

# --- Configurações ---
athena = boto3.client("athena", region_name="us-east-1")
s3 = boto3.client("s3")

DATABASE = "painel-monitoracao"
WORKGROUP = "painel_monitoracao"
OUTPUT_BUCKET = "painel-monitoracao-administrativo-athena"
CACHE_BUCKET = "painel-monitoracao-administrativo-dados"
CACHE_KEY = "api_cache/dados.json"

QUERY = """
SELECT 
    num_chamado,
    equipe,
    gerente_equipe,
    dpto_equipe,
    coord_dpto_equipe,
    ger_dpto_equipe,
    dpto_ger_equipe,
    severidade,
    dat_abertura,
    dat_status_concluido,
    dat_fechamento,
    data_normalizacao,
    status,
    classificacao,
    dat_estouro_sla,
    num_duracao,
    duracao_sla,
    titulo,
    sla_violado
FROM painel_monitoracao_administrativo
WHERE dat_abertura >= date_add('month', -6, current_date)
  AND (
        (classificacao = 'Incidente' AND severidade IN ('3 - Média', '4 - Alta', '5 - Crítica'))
     OR (classificacao = 'Solicitação' AND severidade = '4 - Alta')
      )
ORDER BY num_chamado DESC
"""

# --- Logger ---
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
    try:
        logger.info("Iniciando execução da query Athena...")

        # Inicia execução no Athena
        exec_id = athena.start_query_execution(
            QueryString=QUERY,
            QueryExecutionContext={"Database": DATABASE},
            ResultConfiguration={"OutputLocation": f"s3://{OUTPUT_BUCKET}/"},
            WorkGroup=WORKGROUP
        )["QueryExecutionId"]

        # Espera até a query finalizar
        while True:
            status = athena.get_query_execution(QueryExecutionId=exec_id)["QueryExecution"]["Status"]["State"]
            if status in ["SUCCEEDED", "FAILED", "CANCELLED"]:
                break
            time.sleep(3)

        if status != "SUCCEEDED":
            raise Exception(f"Athena query failed: {status}")

        logger.info("Consulta Athena concluída com sucesso.")

        # Athena salva o resultado CSV automaticamente no bucket de OUTPUT
        result_path = f"{exec_id}.csv"
        logger.info(f"Lendo arquivo CSV: {result_path}")

        # Lê o CSV diretamente do S3
        obj = s3.get_object(Bucket=OUTPUT_BUCKET, Key=result_path)
        content = obj["Body"].read().decode("utf-8")
        csv_reader = csv.DictReader(io.StringIO(content))

        rows = list(csv_reader)
        logger.info(f"Total de registros lidos: {len(rows)}")

        # Salva no bucket de cache em formato JSON
        s3.put_object(
            Bucket=CACHE_BUCKET,
            Key=CACHE_KEY,
            Body=json.dumps(rows, ensure_ascii=False),
            ContentType="application/json"
        )

        logger.info(f"Arquivo JSON salvo com sucesso em s3://{CACHE_BUCKET}/{CACHE_KEY}")
        return {"status": "ok", "rows": len(rows)}

    except Exception as e:
        logger.exception("Erro ao processar Lambda:")
        return {"error": str(e)}
